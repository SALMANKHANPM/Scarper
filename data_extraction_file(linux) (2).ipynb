{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa557f57-9d68-4a3f-9d3f-a14639250212",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install selenium duckduckgo_search beautifulsoup4  webdriver_manager pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d3e2f5e5-71c2-493b-b8ed-fe5234e4f069",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from duckduckgo_search import DDGS\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3e43dd69-7a2d-4e7f-af97-f0a5e9dbc13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(name, url):\n",
    "    # Configure Selenium options (headless browser, if desired)\n",
    "    options = Options()\n",
    "    options.headless = True  # Set to False if you want to see the browser interaction\n",
    "\n",
    "    # Specify the path to chromedriver.exe\n",
    "    # driver_path = 'chromedriver.exe'  # Update with your actual path to chromedriver\n",
    "\n",
    "    # # Initialize the WebDriver with Chrome\n",
    "    # service = Service(driver_path)\n",
    "    # driver = webdriver.Chrome(service=service, options=options)\n",
    "    options = Options()\n",
    "# options.add_argument('--headless')\n",
    "# options.add_argument('--no-sandbox')\n",
    "    options.add_argument('--disable-dev-shm-usage')\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "\n",
    "    # Load the webpage\n",
    "    driver.get(url)\n",
    "\n",
    "    # Wait for the page to fully render (adjust wait time as needed)\n",
    "    time.sleep(5)  # Example: Wait for 5 seconds (you can use more sophisticated waits)\n",
    "\n",
    "    # Extract the page content after rendering\n",
    "    page_source = driver.page_source\n",
    "\n",
    "    # Parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "    # Close the WebDriver session\n",
    "    driver.quit()\n",
    "\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4ce559d5-585d-41a5-ab50-e888451b152a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Remove extra whitespace and newlines\n",
    "    return re.sub(r'\\s+', ' ', text).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1df7358c-9d19-43bf-a283-c749d641294c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_element(element):\n",
    "    if element.name in ['p', 'ul', 'ol', 'table']:\n",
    "        return clean_text(element.get_text())\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f47fc279-0550-405a-8492-b8b4a5aa0b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape(soup):\n",
    "\n",
    "    # Remove all script and style elements\n",
    "    for script in soup([\"script\", \"style\", \"img\", \"link\", \"href\"]):\n",
    "        script.decompose()\n",
    "\n",
    "    parse_data = {\n",
    "        \"title\": \"\",\n",
    "        \"meta_description\": \"\",\n",
    "        \"keywords\": \"\",\n",
    "        \"basic_info\": {},\n",
    "    }\n",
    "\n",
    "    # Extract title\n",
    "    title_tag = soup.find('title')\n",
    "    if title_tag:\n",
    "        parse_data[\"title\"] = clean_text(title_tag.string)\n",
    "\n",
    "    # Extract meta description\n",
    "    meta_desc = soup.find('meta', attrs={'name': 'description'})\n",
    "    if meta_desc:\n",
    "        parse_data[\"meta_description\"] = clean_text(meta_desc.get('content', ''))\n",
    "\n",
    "    # Extract keywords\n",
    "    keywords = soup.find('meta', attrs={'name': 'keywords'})\n",
    "    if keywords:\n",
    "        parse_data[\"keywords\"] = clean_text(keywords.get('content', ''))\n",
    "\n",
    "    # Extract structured data\n",
    "    structured_data_tags = soup.find_all('script', type='application/ld+json')\n",
    "    for tag in structured_data_tags:\n",
    "        try:\n",
    "            data = json.loads(tag.string)\n",
    "            parse_data[\"structured_data\"].append(data)\n",
    "        except json.JSONDecodeError:\n",
    "            pass  # Ignore invalid JSON\n",
    "\n",
    "    # Extract basic information from the first section\n",
    "    parse_data[\"basic_info\"]=[]\n",
    "    parse_data[\"Section\"] = {}\n",
    "    about_section = soup.find('section', class_='bg-white rounded-16 p-6')\n",
    "    if about_section:\n",
    "        parse_data[\"Section\"][\"name\"] = clean_text(about_section.find('h2').text) if about_section.find('h2') else \"\"\n",
    "        content_div = about_section.find('div', class_='content-section')\n",
    "        if content_div:\n",
    "            paragraphs = content_div.find_all('p')\n",
    "            parse_data[\"Section\"][\"description\"] = \"\\n\".join([clean_text(p.text) for p in paragraphs])\n",
    "            \n",
    "            # Find all headers (h2, h3, h4, etc.) in the content div\n",
    "            headers = content_div.find_all(['h2', 'h3', 'h4', 'h5', 'h6'])\n",
    "            \n",
    "            for header in headers:\n",
    "                header_text = clean_text(header.text)\n",
    "                description = []\n",
    "                \n",
    "                # Collect all paragraph siblings until the next header or end of content\n",
    "                for sibling in header.find_next_siblings():\n",
    "                    if sibling.name in ['h2', 'h3', 'h4', 'h5', 'h6']:\n",
    "                        break\n",
    "                    if sibling.name == 'p':\n",
    "                        description.append(extract_text_from_element(sibling))\n",
    "                \n",
    "                parse_data[\"basic_info\"].append({\n",
    "                    \"name\": header_text,\n",
    "                    \"description\": [item for item in description if item]\n",
    "                })\n",
    "                \n",
    "    \n",
    "    # extract tabular data\n",
    "    tables = soup.find_all('table', class_=lambda x: x is not None and x.strip() != \"\")\n",
    "    for table in tables:\n",
    "        header = table.find_previous('h2')\n",
    "        header_text = header.text.strip() if header else \"No header found\"\n",
    "\n",
    "        # Extract headers\n",
    "        headers = [th.text.strip() for th in table.find_all('th')]\n",
    "\n",
    "        # Initialize the result list\n",
    "        table_data = []\n",
    "\n",
    "        # Track rowspans\n",
    "        rowspan_tracker = [0] * len(headers)\n",
    "\n",
    "        for row in table.find_all('tr')[1:]:  # Skip the header row\n",
    "            cells = row.find_all(['td', 'th'])\n",
    "            row_data = {}\n",
    "            col_index = 0\n",
    "\n",
    "            for cell in cells:\n",
    "                while col_index < len(headers) and rowspan_tracker[col_index] > 0:\n",
    "                    if table_data:\n",
    "                        row_data[headers[col_index]] = table_data[-1].get(headers[col_index], \"\")\n",
    "                    rowspan_tracker[col_index] -= 1\n",
    "                    col_index += 1\n",
    "\n",
    "                if col_index < len(headers):\n",
    "                    rowspan = int(cell.get('rowspan', 1))\n",
    "                    colspan = int(cell.get('colspan', 1))\n",
    "\n",
    "                    for _ in range(colspan):\n",
    "                        if col_index < len(headers):\n",
    "                            row_data[headers[col_index]] = cell.text.strip()\n",
    "                            if rowspan > 1:\n",
    "                                rowspan_tracker[col_index] = rowspan - 1\n",
    "                            col_index += 1\n",
    "\n",
    "            if row_data:  # Avoid adding empty rows\n",
    "                table_data.append(row_data)\n",
    "\n",
    "        parse_data[header_text] = table_data\n",
    "\n",
    "\n",
    "    \n",
    "    # Initialize an empty list to store the FAQs\n",
    "    faqs = []\n",
    "    # Find all the question and answer pairs\n",
    "    faq_div = soup.find('div', class_='cdcms_faqs')\n",
    "    if faq_div:\n",
    "        questions = faq_div.find_all('p', class_='accordio')\n",
    "        answers = faq_div.find_all('div', class_='liv')\n",
    "\n",
    "        # Iterate through the questions and answers and store them in a structured format\n",
    "        for question, answer in zip(questions, answers):\n",
    "            faq_item = {\n",
    "                'question': question.get_text(strip=True).replace(\"Ques. \", \"\"),\n",
    "                'answer': answer.get_text(strip=True).replace(\"Ans. \", \"\")\n",
    "            }\n",
    "            faqs.append(faq_item)\n",
    "    parse_data['faqs']=faqs\n",
    "    return parse_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5707507d-1780-400e-8b68-94663e8428d6",
   "metadata": {},
   "source": [
    "name = 'SRM'\n",
    "url = 'https://collegedunia.com/university/25896-srm-institute-of-science-and-technology-srmist-chennai'\n",
    "\n",
    "extracted = extract(name, url)\n",
    "Scraper(extracted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "19f63ee5-cb06-47da-88d9-523a0ac174d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_college_on_duckduckgo(college_name):\n",
    "    query = f\"{college_name} site:collegedunia.com\"\n",
    "    \n",
    "    # Create a DDGS object and perform a search\n",
    "    with DDGS() as ddgs:\n",
    "        results = ddgs.text(query, max_results=1)\n",
    "    res=[]    \n",
    "    if results:\n",
    "        for result in results:\n",
    "            res.append(result['href'])\n",
    "        return res\n",
    "    else:\n",
    "        return \"No CollegeDunia link found in the search results.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4860034-5dca-42b8-8218-bef35d9acf30",
   "metadata": {},
   "source": [
    "name = 'college_names6.csv'\n",
    "df = pd.read_csv(name)\n",
    "df['College Name'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "176e74d0-e20e-48de-895e-4c3dda731fd5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NoSuchWindowException",
     "evalue": "Message: no such window: target window already closed\nfrom unknown error: web view not found\n  (Session info: chrome=128.0.6613.86)\nStacktrace:\n\tGetHandleVerifier [0x00007FF6DDD79632+30946]\n\t(No symbol) [0x00007FF6DDD2E3C9]\n\t(No symbol) [0x00007FF6DDC26FDA]\n\t(No symbol) [0x00007FF6DDBFCB85]\n\t(No symbol) [0x00007FF6DDCA37A7]\n\t(No symbol) [0x00007FF6DDCBA771]\n\t(No symbol) [0x00007FF6DDC9C813]\n\t(No symbol) [0x00007FF6DDC6A6E5]\n\t(No symbol) [0x00007FF6DDC6B021]\n\tGetHandleVerifier [0x00007FF6DDEAF83D+1301229]\n\tGetHandleVerifier [0x00007FF6DDEBBDB7+1351783]\n\tGetHandleVerifier [0x00007FF6DDEB2A03+1313971]\n\tGetHandleVerifier [0x00007FF6DDDADD06+245686]\n\t(No symbol) [0x00007FF6DDD3758F]\n\t(No symbol) [0x00007FF6DDD33804]\n\t(No symbol) [0x00007FF6DDD33992]\n\t(No symbol) [0x00007FF6DDD2A3EF]\n\tBaseThreadInitThunk [0x00007FFE2C44257D+29]\n\tRtlUserThreadStart [0x00007FFE2D6CAF28+40]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNoSuchWindowException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[58], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m web \u001b[38;5;129;01min\u001b[39;00m search_college_on_duckduckgo(name):\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m web:\n\u001b[1;32m----> 5\u001b[0m         extracted \u001b[38;5;241m=\u001b[39m \u001b[43mextract\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m      7\u001b[0m         \u001b[38;5;28mprint\u001b[39m(name)\n",
      "Cell \u001b[1;32mIn[33], line 20\u001b[0m, in \u001b[0;36mextract\u001b[1;34m(name, url)\u001b[0m\n\u001b[0;32m     17\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m5\u001b[39m)  \u001b[38;5;66;03m# Example: Wait for 5 seconds (you can use more sophisticated waits)\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Extract the page content after rendering\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m page_source \u001b[38;5;241m=\u001b[39m \u001b[43mdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpage_source\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Parse the HTML content using BeautifulSoup\u001b[39;00m\n\u001b[0;32m     23\u001b[0m soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(page_source, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mN:\\SIH\\pythonProject\\.venv\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:455\u001b[0m, in \u001b[0;36mWebDriver.page_source\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    446\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m    447\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpage_source\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m    448\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Gets the source of the current page.\u001b[39;00m\n\u001b[0;32m    449\u001b[0m \n\u001b[0;32m    450\u001b[0m \u001b[38;5;124;03m    :Usage:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;124;03m            driver.page_source\u001b[39;00m\n\u001b[0;32m    454\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 455\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCommand\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGET_PAGE_SOURCE\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mN:\\SIH\\pythonProject\\.venv\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:354\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    352\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_executor\u001b[38;5;241m.\u001b[39mexecute(driver_command, params)\n\u001b[0;32m    353\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[1;32m--> 354\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_handler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    355\u001b[0m     response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unwrap_value(response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    356\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mN:\\SIH\\pythonProject\\.venv\\Lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py:229\u001b[0m, in \u001b[0;36mErrorHandler.check_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    227\u001b[0m         alert_text \u001b[38;5;241m=\u001b[39m value[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malert\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[38;5;66;03m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[1;32m--> 229\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[1;31mNoSuchWindowException\u001b[0m: Message: no such window: target window already closed\nfrom unknown error: web view not found\n  (Session info: chrome=128.0.6613.86)\nStacktrace:\n\tGetHandleVerifier [0x00007FF6DDD79632+30946]\n\t(No symbol) [0x00007FF6DDD2E3C9]\n\t(No symbol) [0x00007FF6DDC26FDA]\n\t(No symbol) [0x00007FF6DDBFCB85]\n\t(No symbol) [0x00007FF6DDCA37A7]\n\t(No symbol) [0x00007FF6DDCBA771]\n\t(No symbol) [0x00007FF6DDC9C813]\n\t(No symbol) [0x00007FF6DDC6A6E5]\n\t(No symbol) [0x00007FF6DDC6B021]\n\tGetHandleVerifier [0x00007FF6DDEAF83D+1301229]\n\tGetHandleVerifier [0x00007FF6DDEBBDB7+1351783]\n\tGetHandleVerifier [0x00007FF6DDEB2A03+1313971]\n\tGetHandleVerifier [0x00007FF6DDDADD06+245686]\n\t(No symbol) [0x00007FF6DDD3758F]\n\t(No symbol) [0x00007FF6DDD33804]\n\t(No symbol) [0x00007FF6DDD33992]\n\t(No symbol) [0x00007FF6DDD2A3EF]\n\tBaseThreadInitThunk [0x00007FFE2C44257D+29]\n\tRtlUserThreadStart [0x00007FFE2D6CAF28+40]\n"
     ]
    }
   ],
   "source": [
    "for l in range(len(df['College Name'])):\n",
    "    name = df['College Name'][l]\n",
    "    for web in search_college_on_duckduckgo(name):\n",
    "        if web:\n",
    "            extracted = extract(name, web)\n",
    "        else:\n",
    "            print(name)\n",
    "        with open(f\"{name}.json\",'w',encoding='utf-8') as js:\n",
    "            json.dump(scrape(extracted), js, ensure_ascii=False, indent=4)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7cff82-6322-4876-b5d2-d358c15efea2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
